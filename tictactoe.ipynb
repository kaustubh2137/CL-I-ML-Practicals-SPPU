{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Board:\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Current Board:\n",
      "[[0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Your Move:\n",
      "Current Board:\n",
      "[[-1  0  1]\n",
      " [ 0  0  0]\n",
      " [ 0  0  0]]\n",
      "Current Board:\n",
      "[[-1  0  1]\n",
      " [ 0  0  0]\n",
      " [ 0  1  0]]\n",
      "Your Move:\n",
      "Current Board:\n",
      "[[-1  0  1]\n",
      " [ 0  0  0]\n",
      " [ 0  1 -1]]\n",
      "Current Board:\n",
      "[[-1  1  1]\n",
      " [ 0  0  0]\n",
      " [ 0  1 -1]]\n",
      "Your Move:\n",
      "You Win!\n",
      "Final Board:\n",
      "[[-1  1  1]\n",
      " [ 0 -1  0]\n",
      " [ 0  1 -1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        # Flatten board state for easier storage in Q-table\n",
    "        return tuple(self.board.flatten())\n",
    "    \n",
    "    def is_winner(self, player):\n",
    "        for i in range(3):\n",
    "            if np.all(self.board[i, :] == player) or np.all(self.board[:, i] == player):\n",
    "                return True\n",
    "        if np.all(np.diag(self.board) == player) or np.all(np.diag(np.fliplr(self.board)) == player):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_draw(self):\n",
    "        return np.all(self.board != 0)\n",
    "    \n",
    "    def available_actions(self):\n",
    "        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == 0]\n",
    "    \n",
    "    def step(self, action, player):\n",
    "        self.board[action] = player\n",
    "        if self.is_winner(player):\n",
    "            return self.get_state(), 1, True  # Win reward\n",
    "        elif self.is_draw():\n",
    "            return self.get_state(), 0, True  # Draw reward\n",
    "        else:\n",
    "            return self.get_state(), -0.1, False  # Slight penalty to continue the game\n",
    "\n",
    "# Q-learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, epsilon=0.1, alpha=0.5, gamma=0.9):\n",
    "        self.q_table = {}\n",
    "        self.epsilon = epsilon  # Exploration factor\n",
    "        self.alpha = alpha      # Learning rate\n",
    "        self.gamma = gamma      # Discount factor\n",
    "    \n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table.get((state, action), 0.0)\n",
    "    \n",
    "    def choose_action(self, state, available_actions):\n",
    "        # Epsilon-greedy policy\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(available_actions)\n",
    "        else:\n",
    "            q_values = [self.get_q_value(state, action) for action in available_actions]\n",
    "            max_q = max(q_values)\n",
    "            # Choose the best action with the highest Q-value\n",
    "            return available_actions[q_values.index(max_q)]\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, next_state, done, available_actions):\n",
    "        # Get max Q-value for the next state\n",
    "        best_next_q = max([self.get_q_value(next_state, a) for a in available_actions], default=0)\n",
    "        q_value = self.get_q_value(state, action)\n",
    "        # Q-learning update rule\n",
    "        if done:\n",
    "            # If game ended, do not consider future reward\n",
    "            self.q_table[(state, action)] = q_value + self.alpha * (reward - q_value)\n",
    "        else:\n",
    "            self.q_table[(state, action)] = q_value + self.alpha * (reward + self.gamma * best_next_q - q_value)\n",
    "\n",
    "# Training the Agent\n",
    "def train(agent, episodes=1000):\n",
    "    env = TicTacToe()\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        player = 1  # Agent plays as '1'\n",
    "        \n",
    "        while not done:\n",
    "            available_actions = env.available_actions()\n",
    "            action = agent.choose_action(state, available_actions)\n",
    "            next_state, reward, done = env.step(action, player)\n",
    "            \n",
    "            if done:\n",
    "                # Update the Q-table for the final move\n",
    "                agent.update_q_table(state, action, reward, next_state, done, available_actions)\n",
    "            else:\n",
    "                # Opponent plays randomly\n",
    "                opponent_action = random.choice(env.available_actions())\n",
    "                next_state, opponent_reward, done = env.step(opponent_action, -player)\n",
    "                \n",
    "                # Update the Q-table for the agent's move\n",
    "                agent.update_q_table(state, action, reward, next_state, done, available_actions)\n",
    "                \n",
    "            state = next_state\n",
    "\n",
    "# Testing the Agent\n",
    "def play(agent):\n",
    "    env = TicTacToe()\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    player = 1  # Agent plays as '1'\n",
    "    \n",
    "    while not done:\n",
    "        print(\"Current Board:\")\n",
    "        print(env.board)\n",
    "        \n",
    "        if player == 1:\n",
    "            available_actions = env.available_actions()\n",
    "            action = agent.choose_action(state, available_actions)\n",
    "            state, reward, done = env.step(action, player)\n",
    "            if done:\n",
    "                if reward == 1:\n",
    "                    print(\"Agent Wins!\")\n",
    "                elif reward == 0:\n",
    "                    print(\"It's a Draw!\")\n",
    "                else:\n",
    "                    print(\"Agent Loses.\")\n",
    "        else:\n",
    "            # Human player input\n",
    "            print(\"Your Move:\")\n",
    "            action = tuple(map(int, input(\"Enter row, column (0-2 for both): \").split(',')))\n",
    "            state, _, done = env.step(action, player)\n",
    "            if done:\n",
    "                print(\"You Win!\")\n",
    "        \n",
    "        player = -player  # Switch player\n",
    "\n",
    "    print(\"Final Board:\")\n",
    "    print(env.board)\n",
    "\n",
    "# Initialize and train the agent\n",
    "agent = QLearningAgent()\n",
    "train(agent, episodes=10000)  # Train for 10,000 episodes for better performance\n",
    "play(agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
